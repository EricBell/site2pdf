# site2pdf ğŸ“„

A powerful Python CLI application that intelligently scrapes websites and generates comprehensive PDF documents with human-like behavior.

## Features

### ğŸ§  **Intelligent Scraping**
- ğŸ¯ **Path-Aware Discovery**: Stays focused on relevant sections (e.g., `/docs/*` only)
- ğŸ·ï¸ **Content Classification**: Distinguishes documentation, content, navigation, and technical pages
- ğŸ” **Smart URL Filtering**: Automatically excludes admin pages, APIs, and irrelevant content
- ğŸ“Š **Quality Assessment**: Analyzes page content quality and skips low-value pages

### ğŸ•µï¸ **Human-Like Behavior** 
- ğŸ­ **Microsoft Edge Simulation**: Realistic browser headers and user agent
- â±ï¸ **Variable Delays**: Human-like reading and decision times (2-8s per page)
- ğŸª **Session Management**: Proper cookie handling and referrer tracking
- ğŸ“ˆ **Adaptive Behavior**: Detects rate limiting and adjusts automatically
- ğŸ˜´ **Fatigue Simulation**: Gradually slower over time like real users

### ğŸ“„ **Advanced PDF Generation**
- ğŸ“– **Documentation Focus**: Prioritizes user-facing content over technical files
- ğŸ–¼ï¸ **Image Embedding**: Downloads and includes images with proper formatting
- ğŸ“š **Table of Contents**: Automatic TOC generation with page links
- ğŸ¨ **Professional Layout**: Clean, readable formatting with proper structure

### ğŸ”§ **Powerful Configuration**
- ğŸ® **Interactive Preview**: Tree-view URL selection with approval system
- ğŸ’¾ **URL List Persistence**: Save and reuse approved URL lists
- âš™ï¸ **Extensive Options**: Fine-tune crawling, delays, content filtering, and output

## Quick Start

### Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd site2pdf
```

2. Create a virtual environment:
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

### Basic Usage

```bash
# Scrape a website and generate PDF
python run.py https://example.com

# With custom options
python run.py https://example.com --output my-site.pdf --max-depth 3 --verbose

# Dry run to see what would be scraped
python run.py https://example.com --dry-run
```

### Command Line Options

- `base_url` (required): The starting URL to scrape
- `--output, -o`: Output PDF filename
- `--max-depth, -d`: Maximum crawl depth
- `--max-pages, -p`: Maximum number of pages to scrape
- `--delay`: Delay between requests (seconds)
- `--config, -c`: Configuration file path
- `--verbose, -v`: Enable verbose logging
- `--dry-run`: Show what would be scraped without doing it
- `--preview`: Interactive URL selection with tree view
- `--exclude`: URL patterns to exclude (can use multiple times)
- `--save-approved`: Save approved URLs to file for reuse
- `--load-approved`: Load previously approved URLs from file

## Configuration

### Configuration File (config.yaml)

The application uses `config.yaml` for public configuration:

```yaml
crawling:
  max_depth: 5
  request_delay: 2.0
  max_pages: 1000

pdf:
  output_filename: "scraped_website.pdf"
  page_size: "A4"
  include_toc: true

# ... see config.yaml for full options
```

### Environment Variables (.env)

Private configuration goes in `.env`:

```bash
# Proxy settings
HTTP_PROXY=http://proxy.company.com:8080

# Custom delays
CRAWL_DELAY_OVERRIDE=3

# Debug mode
DEBUG_MODE=true
```

## Project Structure

```
site2pdf/
â”œâ”€â”€ requirements.md          # Detailed requirements
â”œâ”€â”€ .env                    # Private configuration
â”œâ”€â”€ config.yaml             # Public configuration
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ run.py                  # Main entry point
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cli.py             # Click CLI interface
â”‚   â”œâ”€â”€ scraper.py         # Web scraping logic
â”‚   â”œâ”€â”€ extractor.py       # Content extraction
â”‚   â”œâ”€â”€ pdf_generator.py   # PDF creation
â”‚   â””â”€â”€ utils.py           # Utility functions
â”œâ”€â”€ output/                # Generated PDFs
â”œâ”€â”€ temp/                  # Temporary files (images)
â””â”€â”€ logs/                  # Log files
```

## Examples

### Basic Website Scraping
```bash
python run.py https://docs.python.org
```

### Limited Depth Scraping
```bash
python run.py https://example.com --max-depth 2 --max-pages 50
```

### Custom Configuration
```bash
python run.py https://example.com --config custom-config.yaml --output custom-name.pdf
```

### Verbose Mode with Custom Delay
```bash
python run.py https://example.com --verbose --delay 3
```

## Advanced Features

### ğŸ® Interactive Preview Mode
Preview and approve URLs before scraping with a tree-view interface:

```bash
# Interactive preview with approval
python run.py https://example.com/docs/ --preview

# Preview with URL filtering
python run.py https://example.com --preview --exclude "/admin" --exclude "/api"

# Save approved URLs for reuse
python run.py https://example.com --preview --save-approved approved_urls.json
```

**Preview Features:**
- ğŸ“Š Content type indicators (ğŸ“– Documentation, ğŸ“„ Content, ğŸ§­ Navigation)
- ğŸ¯ Path scoping information showing allowed/blocked sections  
- ğŸŒ³ Hierarchical tree view of discovered URLs
- âœ… Interactive approval with exclude/include commands
- ğŸ’¾ Save/load approved URL lists for repeated scraping

### ğŸ¯ Path-Aware Scoping
Automatically stays within relevant sections of websites:

```bash
# Starting from documentation section
python run.py https://example.com/docs/getting-started/

# Will scrape:
# âœ… /docs/api-reference/     (same section)
# âœ… /docs/                   (parent section) 
# âœ… /                        (homepage)

# Will ignore:
# âŒ /blog/                   (different section)
# âŒ /admin/                  (admin area)
# âŒ /xmlrpc.php             (technical file)
```

**Configuration:**
```yaml
path_scoping:
  enabled: true                    # Enable path-based scoping
  allow_parent_levels: 1           # Allow 1 level up from starting path
  allow_homepage: true             # Always allow homepage
  allow_siblings: true             # Allow sibling paths in same section
  allow_navigation: "limited"      # Navigation link policy
```

### ğŸ•µï¸ Human-Like Behavior
Mimics real user browsing patterns to avoid detection:

```bash
# Enable human-like delays and behavior
python run.py https://example.com --verbose
```

**Behavioral Features:**
- ğŸ­ **Microsoft Edge simulation** with realistic headers
- â±ï¸ **Variable delays**: 2-8 seconds reading + 1-3 seconds decision time
- ğŸ“ˆ **Adaptive timing**: Slower for complex pages, faster for simple ones
- ğŸ˜´ **Fatigue simulation**: Gradually slower over long sessions
- ğŸª **Session management**: Proper cookies and referrer tracking
- ğŸ“Š **Rate limit detection**: Automatically backs off when detected

**Configuration:**
```yaml
human_behavior:
  delays:
    base_reading_time: [2, 8]      # Time to "read" page content
    navigation_decision: [1, 3]    # Time to "decide" next action
    variance_percent: 30           # Random timing variation
  browsing:
    session_break_after: 50        # Break every N pages
    weekend_factor: 1.2            # 20% slower on weekends
```

### ğŸ” Smart Content Classification
Automatically identifies and prioritizes different types of content:

- ğŸ“– **Documentation**: `/docs/`, `/help/`, `/guide/`, `/tutorial/`
- ğŸ“„ **Content Pages**: `/about/`, `/blog/`, `/features/`, `/examples/`
- ğŸ§­ **Navigation**: Homepage, sitemaps, main navigation
- âŒ **Excluded**: Admin pages, APIs, technical files, login forms

**Quality Assessment:**
- Word count and content depth analysis
- Heading structure evaluation  
- Image and multimedia content detection
- Low-quality page filtering (too sparse or too navigation-heavy)

## Output

The application generates:

- **PDF Document**: Comprehensive PDF with all scraped content
- **Log Files**: Detailed crawling and processing logs in `logs/`
- **Temporary Files**: Downloaded images stored in `temp/` during processing

## PDF Features

- âœ… Table of contents with page links
- âœ… Preserved content structure and formatting
- âœ… Embedded images with captions
- âœ… Page numbers and metadata
- âœ… Professional styling and layout
- âœ… Link references for each page

## Compliance & Ethics

- ğŸ¤– Respects robots.txt files
- â±ï¸ Implements configurable crawl delays
- ğŸš« Filters out admin and login pages
- ğŸ“Š Reasonable default limits
- ğŸ”’ Handles HTTP errors gracefully

## Troubleshooting

### Common Issues

1. **Permission Errors**: Ensure output directories are writable
2. **Network Timeouts**: Increase timeout in config.yaml
3. **Memory Issues**: Reduce max_pages for large sites
4. **PDF Generation Fails**: Check WeasyPrint dependencies

### Debugging

Enable verbose mode and check logs:
```bash
python run.py https://example.com --verbose
cat logs/scraper.log
```

## Dependencies

- **requests**: HTTP requests and web scraping
- **beautifulsoup4**: HTML parsing
- **click**: Command-line interface
- **weasyprint**: PDF generation
- **python-dotenv**: Environment variables
- **PyYAML**: Configuration files
- **tqdm**: Progress bars
- **Pillow**: Image processing

## License

MIT License - see LICENSE file for details.

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## Support

For issues and questions:
- Check the logs in `logs/scraper.log`
- Review configuration in `config.yaml`
- Refer to `requirements.md` for detailed specifications